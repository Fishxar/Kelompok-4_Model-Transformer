{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62E7gKwo3yMB",
        "outputId": "954d46bc-a376-4bdd-ef59-b0a0d38ae7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch pandas scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "13gS1INQ33di"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv('Binary_Class_Balanced_Dataset_4Lac_60_Features.csv')"
      ],
      "metadata": {
        "id": "vHz0VKHR34zr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv('Binary_Class_Balanced_Dataset_4Lac_60_Features.csv')\n",
        "\n",
        "# Print the column names\n",
        "print(data.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNYnY7Tj6fxg",
        "outputId": "93b91539-622b-4421-90b0-ca987ff2fa41"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',\n",
            "       'Total Length of Fwd Packets', 'Total Length of Bwd Packets',\n",
            "       'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
            "       'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
            "       'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
            "       'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s',\n",
            "       'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max',\n",
            "       'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
            "       'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',\n",
            "       'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags',\n",
            "       'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s',\n",
            "       'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length',\n",
            "       'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance',\n",
            "       'FIN Flag Count', 'SYN Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
            "       'URG Flag Count', 'Down/Up Ratio', 'Average Packet Size',\n",
            "       'Avg Fwd Segment Size', 'Avg Bwd Segment Size',\n",
            "       'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'act_data_pkt_fwd',\n",
            "       'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max',\n",
            "       'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan semua fitur kecuali 'Label' sebagai input\n",
        "X = data.drop(columns=['Label']).values  # Menghapus kolom 'Label'\n",
        "y = data['Label'].values  # Mengambil kolom 'Label' sebagai target"
      ],
      "metadata": {
        "id": "dOHQ8btV34wd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset menjadi training dan validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ngnc9J-034tL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardisasi fitur\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)"
      ],
      "metadata": {
        "id": "Xi8WDT5234qH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.features[idx]\n",
        "        label = self.labels[idx]\n",
        "        return {'input': torch.tensor(feature, dtype=torch.float32), 'label': torch.tensor(label)}\n"
      ],
      "metadata": {
        "id": "BS9RW2BA34nF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader\n",
        "train_dataset = TabularDataset(X_train, y_train)\n",
        "val_dataset = TabularDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "IzKSokjh34kI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)  # Layer pertama\n",
        "        self.fc2 = nn.Linear(128, 64)           # Layer kedua\n",
        "        self.fc3 = nn.Linear(64, num_classes)   # Layer output\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "1nSJZJ6T34hv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "input_size = X_train.shape[1]  # Jumlah fitur\n",
        "num_classes = len(set(y))       # Jumlah kelas\n",
        "model = MLP(input_size, num_classes)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function untuk klasifikasi\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "JPO58nEb34VX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(10):  # number of epochs\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = batch['input'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2_u06mK34UB",
        "outputId": "0179f169-639b-4e1e-f4c6-6fb1121ced85"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.1208152249455452\n",
            "Epoch 2, Loss: 0.12760938704013824\n",
            "Epoch 3, Loss: 0.06483794748783112\n",
            "Epoch 4, Loss: 5.975149633741239e-06\n",
            "Epoch 5, Loss: 0.058578409254550934\n",
            "Epoch 6, Loss: 2.0116412997595035e-06\n",
            "Epoch 7, Loss: 0.03256457298994064\n",
            "Epoch 8, Loss: 0.00020960108668077737\n",
            "Epoch 9, Loss: 0.08911853283643723\n",
            "Epoch 10, Loss: 0.05720480531454086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'mlp_model.pth')"
      ],
      "metadata": {
        "id": "TMBUUljS8ZHS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer Model Bert**\n",
        "\n",
        "**1. Pendahuluan**\n",
        "\n",
        "Proyek ini bertujuan untuk membangun model klasifikasi menggunakan model transformer BERT (Bidirectional Encoder Representations from Transformers). Dataset yang digunakan adalah dataset yang berisi fitur-fitur numerik yang akan diolah menjadi representasi teks. Model ini bertujuan untuk memprediksi label berdasarkan input yang diberikan.\n",
        "\n",
        "**2. Persiapan Data**\n",
        "\n",
        "**2.1 Memuat Dataset**\n",
        "\n",
        "Dataset berhasil dimuat menggunakan Pandas, dan semua data yang diperlukan tersedia untuk proses selanjutnya.\n",
        "\n",
        "**2.2 Pengolahan Data**\n",
        "\n",
        " Pengolahan data dilakukan untuk menyiapkan format input yang diperlukan oleh BERT. Meskipun fitur numerik diubah menjadi teks untuk keperluan ini, penting untuk memastikan bahwa pendekatan ini relevan dengan konteks aplikasi yang diinginkan.\n",
        "\n",
        " **2.3 Pemisahan Fitur dan Label**\n",
        "\n",
        " Fitur dan label berhasil dipisahkan, sehingga siap untuk digunakan dalam pelatihan model.\n",
        "\n",
        "\n",
        "**2.4 Encoding Label**\n",
        "\n",
        "Label dikodekan ke dalam format numerik yang memungkinkan model untuk memahami kategori yang harus diprediksi, sehingga mempersiapkan data untuk proses pelatihan.\n",
        "\n",
        "**2.5 Pembagian Dataset**\n",
        "\n",
        "Dataset berhasil dibagi menjadi data latih dan validasi, yang penting untuk menilai kinerja model pada data yang tidak terlihat selama pelatihan.\n",
        "\n",
        "**3. Tokenisasi**\n",
        "\n",
        "Tokenisasi menggunakan tokenizer BERT berhasil dilakukan, yang mengubah teks menjadi representasi yang dapat dipahami oleh model BERT. Ini adalah langkah kunci dalam mempersiapkan data untuk proses pelatihan.\n",
        "\n",
        "**4. Membuat Dataset Kustom**\n",
        "\n",
        "Kelas dataset kustom berhasil dibuat, memungkinkan pengelolaan data input dan label dengan efisien. Ini memfasilitasi proses pembelajaran model dengan cara yang lebih terstruktur.\n",
        "\n",
        "**5. DataLoader**\n",
        "\n",
        "DataLoader disiapkan untuk mempermudah pengambilan batch data selama pelatihan, meningkatkan efisiensi dan kecepatan proses pelatihan model.\n",
        "\n",
        "**6. Membangun Model**\n",
        "\n",
        "Model BERT berhasil diinisialisasi dengan parameter yang sesuai untuk tugas klasifikasi, menyediakan fondasi yang kuat untuk pembelajaran.\n",
        "\n",
        "**7. Optimizer**\n",
        "\n",
        "Optimizer AdamW dipilih untuk meningkatkan efisiensi proses pembelajaran, yang diharapkan dapat mempercepat konvergensi model.\n",
        "\n",
        "**8. Pelatihan Model**\n",
        "\n",
        "Model dilatih dengan sukses melalui beberapa epoch, dan proses pelatihan menghasilkan penurunan nilai loss, menunjukkan bahwa model belajar dari data. Pemantauan loss selama pelatihan membantu dalam mengidentifikasi kemajuan model.\n",
        "\n",
        "**9. Menyimpan Model**\n",
        "\n",
        "Model yang telah dilatih disimpan dengan baik untuk digunakan di masa mendatang. Ini memungkinkan pemanfaatan model tanpa perlu melatih ulang, menghemat waktu dan sumber daya.\n",
        "\n",
        "**10. Hasil dan Analisis**\n",
        "\n",
        "Hasil pelatihan menunjukkan bahwa model dapat belajar dengan baik, dengan penurunan nilai loss dari epoch ke epoch. Namun, evaluasi lebih lanjut diperlukan untuk memahami seberapa baik model ini bekerja pada data baru dan untuk mengeksplorasi metrik evaluasi yang lebih mendalam.\n",
        "\n",
        "**10.1 Evaluasi Model**\n",
        "\n",
        "Evaluasi model sangat penting untuk menilai kinerja dan akurasi model. Metrik seperti akurasi, presisi, dan recall dapat memberikan wawasan tambahan tentang bagaimana model beroperasi dalam konteks klasifikasi.\n",
        "\n",
        "**11. Kesimpulan**\n",
        "\n",
        "Model klasifikasi berbasis BERT berhasil dibangun dan menunjukkan potensi yang baik dalam memprediksi label berdasarkan input teks. Meskipun penggunaan data numerik dalam format teks berfungsi untuk tujuan ini, perlu dicatat bahwa penggunaan data yang lebih relevan dan representatif akan sangat membantu dalam meningkatkan performa model. Tuning hyperparameter dan eksplorasi lebih lanjut terhadap data dapat dilakukan untuk mencapai hasil yang lebih baik.\n",
        "\n"
      ],
      "metadata": {
        "id": "Cn5lIQs696-P"
      }
    }
  ]
}